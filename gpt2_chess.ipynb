{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# =========================================================\n",
        "# H100 OPTIMIZED CHESS TRANSFORMER (ONE CELL)\n",
        "# =========================================================\n",
        "\n",
        "!pip install -q torch transformers datasets python-chess accelerate tqdm\n",
        "\n",
        "import torch, chess, os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MODEL_NAME = \"gpt2-xl\"        # 1.5B params (H100 friendly)\n",
        "MAX_GAMES = 1_000_000         # start with 1M (scale later)\n",
        "MAX_LEN = 1024                # longer context\n",
        "OUTPUT_DIR = \"/root\"\n",
        "\n",
        "# ---------------- CHECK GPU ----------------\n",
        "assert torch.cuda.is_available(), \"CUDA not available\"\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "dataset = load_dataset(\n",
        "    \"angeluriot/chess_games\",\n",
        "    split=f\"train[:{MAX_GAMES}]\"\n",
        ")\n",
        "\n",
        "def to_text(game):\n",
        "    if game[\"winner\"] == \"white\":\n",
        "        result = \"1-0\"\n",
        "    elif game[\"winner\"] == \"black\":\n",
        "        result = \"0-1\"\n",
        "    else:\n",
        "        result = \"1/2-1/2\"\n",
        "    moves = \" \".join(game[\"moves_san\"])\n",
        "    return f'[Result \"{result}\"] {moves}'\n",
        "\n",
        "dataset = dataset.map(lambda g: {\"text\": to_text(g)})\n",
        "dataset = dataset.remove_columns(dataset.column_names[:-1])\n",
        "\n",
        "# ---------------- TOKENIZER ----------------\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "dataset.set_format(\"torch\")\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.cuda()\n",
        "\n",
        "# ---------------- TRAINING ARGS (H100 OPTIMIZED) ----------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,                 # large dataset \u2192 1 epoch\n",
        "    per_device_train_batch_size=4,       # H100 VRAM\n",
        "    gradient_accumulation_steps=4,       # effective batch = 16\n",
        "    learning_rate=3e-5,\n",
        "    bf16=True,                           # \ud83d\udd25 H100 native\n",
        "    fp16=False,\n",
        "    logging_steps=1000,\n",
        "    save_steps=5000,\n",
        "    save_total_limit=3,\n",
        "    dataloader_num_workers=8,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "# ---------------- TRAIN ----------------\n",
        "trainer.train()\n",
        "\n",
        "# ---------------- SAVE FINAL MODEL ----------------\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\nModel saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "# ---------------- GENERATE SAMPLE GAME ----------------\n",
        "prompt = '[Result \"1-0\"]'\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_length=400,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\nRAW GENERATED GAME:\\n\", generated)\n",
        "\n",
        "# ---------------- FILTER ILLEGAL MOVES ----------------\n",
        "def filter_illegal(pgn):\n",
        "    board = chess.Board()\n",
        "    legal = []\n",
        "    for token in pgn.split():\n",
        "        try:\n",
        "            board.push(board.parse_san(token))\n",
        "            legal.append(token)\n",
        "        except:\n",
        "            break\n",
        "    return \" \".join(legal)\n",
        "\n",
        "print(\"\\nLEGAL GAME ONLY:\\n\", filter_illegal(generated))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f33c45a285d43ebb93340bb81184323",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "README.md: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c213f0d7968c455cafebf09134c3bdff",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "dataset.parquet:   0%|          | 0.00/7.31G [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9a280ec75ee4e5aad5ad89d9f5c9b3a",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "555a8572afb14e32885cf25adcc22e8e",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e40508fb475d4565937316a6453c5470",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c4c20afaa2e4d959d9fb993fc61e10b",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ce21b01207946d38e7a6ff4d8e41727",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "999a82a79bde489db422149d72a57a7f",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18856bd2814a42bf91e3afa3a710b615",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c61c012333549358395cb3811b58b29",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# =========================================================\n",
        "# PHASE-1 CHESS TRANSFORMER (20k LICHESS DATASET) - FIXED\n",
        "# =========================================================\n",
        "\n",
        "!pip install -q torch transformers datasets pandas python-chess tqdm\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import chess\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"gpt2-medium\"\n",
        "CSV_PATH = \"/root/games.csv\"\n",
        "MAX_LEN = 512\n",
        "OUTPUT_DIR = \"/root/chess_phase1\"\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "def format_game(row):\n",
        "    if row[\"winner\"] == \"white\":\n",
        "        result = \"1-0\"\n",
        "    elif row[\"winner\"] == \"black\":\n",
        "        result = \"0-1\"\n",
        "    else:\n",
        "        result = \"1/2-1/2\"\n",
        "    return f'[Result \"{result}\"] {row[\"moves\"]}'\n",
        "\n",
        "df[\"text\"] = df.apply(format_game, axis=1)\n",
        "dataset = Dataset.from_pandas(df[[\"text\"]])\n",
        "\n",
        "# ---------------- TOKENIZER ----------------\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize(batch):\n",
        "    tokens = tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # \u2705 CRITICAL FIX\n",
        "    return tokens\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "dataset.set_format(\"torch\")\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------- TRAINING ----------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=200,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"Model saved to:\", OUTPUT_DIR)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3069fae26fe8456fa4d52dc138844f82",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Map:   0%|          | 0/20058 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='3762' max='3762' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3762/3762 23:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.003200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.576100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.545900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.527600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.519400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.504800</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.499900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.488000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.480700</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.471100</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.467600</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.459300</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.467800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.452000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.455100</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.438900</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.447500</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.449000</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /root/chess_phase1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}